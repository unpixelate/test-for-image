{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TaxiCodeFull.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python379jvsc74a57bd003cc6c235735ef41ce6e0195c89775c2a43a56af6ff61cfa3ad0c5118b18cb89",
      "display_name": "Python 3.7.9 64-bit ('qe-mini-example': conda)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "source": [
        "## Reinforcement learning\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b>Learning outcomes:</b>\n",
        "    <br>\n",
        "    <ul>\n",
        "        <li> Introduction to the theories of reinforcement learning.</li>\n",
        "        <li> Exploration of how Q-learning works</li>\n",
        "        <li> What is the next step after Q-learning </li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Most of us would probably have heard of AI learning to play computer games on its own. A very popular example would be Deepmind. Deepmind took the world by surprise when its AlphaGo program won the Go world champion. In recent times, AI have been able to defeat human players in strategy game. One such example would be OpenAI's AlphaStar. Here, the difficulty is compounded as such game require long term strategic planning.\n",
        "\n",
        "Dario \"TLO\" Wünsch, a professional StarCraft player remarked \"I’ve found AlphaStar’s gameplay incredibly impressive – the system is very skilled at assessing its strategic position, and knows exactly when to engage or disengage with its opponent. And while AlphaStar has excellent and precise control, it doesn’t feel superhuman – certainly not on a level that a human couldn’t theoretically achieve. Overall, it feels very fair – like it is playing a ‘real’ game of StarCraft.\""
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Reinforcement learning analogy\n",
        "\n",
        "Consider the scenario of teaching a dog new tricks. The dog doesn't understand human language, so we can't tell him what to do. Instead, we can create a situation or a cue, and the dog tries to behave in different ways. If the dog's response is desired, we reward them with their favourite snack. Now guess what, the next time the dog is exposed to the same situation, the dog executes a similar action with even more enthusiasm in expectation of more food. That's like learning \"what to do\" from positive experiences. Similarly, dogs will tend to learn what not to do when face with negative experiences. For example, whenever the dog behave undesirably, we would admonish him. This helps the dog to understand and reinforce behavior that are desirable. At the same time, the dog would avoid undesirable behavior.\n",
        "\n",
        "That's exactly how Reinforcement Learning works in a broader sense:\n",
        "\n",
        "Your dog is an \"agent\" that is exposed to the environment. The environment could in your house, with you.\n",
        "The situations they encounter are analogous to a state. An example of a state could be your dog standing and you use a specific word in a certain tone in your living room\n",
        "\n",
        "Our agents react by performing an action to transition from one \"state\" to another \"state,\" your dog goes from standing to sitting, for example.\n",
        "After the transition, they may receive a reward or penalty in return. You give them a treat! Or a \"No\" as a penalty. The policy is the strategy of choosing an action given a state in expectation of better outcomes.\n",
        "\n",
        "Here are some points to take note of:\n",
        "\n",
        "- Greedy (pursuit of current rewards) is not always good.\n",
        "    - There are things that are easy to do for instant gratification, and there's things that provide long term rewards The goal is to not be greedy by looking for the quick immediate rewards, but instead to optimize for maximum rewards over the whole training.\n",
        "\n",
        "- Sequence matters in Reinforcement Learning\n",
        "    - The reward agent does not just depend on the current state, but the entire history of states. Unlike supervised, time step and sequence of state-action-reward is important here."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Q-table\n",
        "\n",
        "In our example below, we will be using OpenAI Gym's Taxi environment"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "import sys\n",
        "sys.tracebacklimit = 0\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "# Init Taxi-V2 Env\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "# Init arbitary values\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.7 # Momemtum 0.2, Current 0.8 Greedy, 0.2 is to reduce volatality and flip flop\n",
        "gamma = 0.2 # Learning Rate 0.1 Greedyness is 10%\n",
        "epsilon = 0.4 # explore 10% exploit 90%\n",
        "\n",
        "\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "training_memory = []\n",
        "\n",
        "for i in range(1, 50000):\n",
        "    state = env.reset()\n",
        "\n",
        "    # Init Vars\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    #training\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon: \n",
        "            # Check the action space\n",
        "            action = env.action_space.sample() # for explore\n",
        "        else:\n",
        "            # Check the learned values\n",
        "            action = np.argmax(q_table[state]) # for exploit\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) #gym generate, the environment already setup for you\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state]) #take highest from q table for exploit\n",
        "\n",
        "        # Update the new value\n",
        "        new_value = (1 - alpha) * old_value + alpha * \\\n",
        "            (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value        \n",
        "        \n",
        "        # penalty for performance evaluation\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "    \n",
        "\n",
        "    if i % 100 == 0:\n",
        "        training_memory.append(q_table.copy())\n",
        "        clear_output(wait=True)\n",
        "        print(\"Episode:\", i)\n",
        "        print(\"Saved q_table during training:\", i)\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(q_table)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7SWys5jC4iEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ffa3f1-a2d1-46eb-a8d6-6b0159f94de2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 49900\nSaved q_table during training: 49900\nTraining finished.\n[[  0.           0.           0.           0.           0.\n    0.        ]\n [ -1.24999956  -1.24999782  -1.24999956  -1.24999782  -1.24998912\n  -10.24999782]\n [ -1.249728    -1.24864     -1.249728    -1.24864     -1.2432\n  -10.24864   ]\n ...\n [ -1.2432      -1.216       -1.2432      -1.24864    -10.2432\n  -10.2432    ]\n [ -1.24998912  -1.2499456   -1.24998912  -1.2499456  -10.24998912\n  -10.24998912]\n [ -0.4         -1.08        -0.4          3.          -9.4\n   -9.4       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyMZPw9F9FwW"
      },
      "source": [
        "** There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends. There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is the taxi), and 4 destination locations. Actions: There are 6 discrete deterministic actions: **\n",
        "\n",
        "    0: move south\n",
        "    1: move north\n",
        "    2: move east\n",
        "    3: move west\n",
        "    4: pickup passenger\n",
        "    5: dropoff passenger\n",
        "Rewards: There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally. Rendering:\n",
        "\n",
        "    blue: passenger\n",
        "    magenta: destination\n",
        "    yellow: empty taxi\n",
        "    green: full taxi\n",
        "    other letters: locations\n",
        "\n",
        "\n",
        "state space is represented by:\n",
        "    (taxi_row, taxi_col, passenger_location, destination)"
      ]
    },
    {
      "source": [
        "Here, the highest number in the array represents the action that the Taxi agent would take"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gi7AXtkCmFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532c0889-77f1-43c6-eb58-62fe488e686f"
      },
      "source": [
        "# At state 499 i will definitely move west\n",
        "state = 499\n",
        "print(training_memory[0][state])\n",
        "print(training_memory[20][state])\n",
        "print(training_memory[50][state])\n",
        "print(training_memory[200][state])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.008     -1.0682761 -1.1004     2.72055   -9.2274    -9.1      ]\n[-0.40000039 -1.07648283 -0.40000128  3.         -9.39958914 -9.39998055]\n[-0.4  -1.08 -0.4   3.   -9.4  -9.4 ]\n[-0.4  -1.08 -0.4   3.   -9.4  -9.4 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ptaEXB2DVhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1736087b-57aa-494d-de68-c0772b7f92ee"
      },
      "source": [
        "# At state 77 i will definitely move east\n",
        "state = 77\n",
        "print(training_memory[0][state])\n",
        "print(training_memory[20][state])\n",
        "print(training_memory[50][state])\n",
        "print(training_memory[200][state])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.07999095 -1.008       3.         -1.08309178 -9.1        -9.18424273]\n[-1.08 -0.4   3.   -1.08 -9.4  -9.4 ]\n[-1.08 -0.4   3.   -1.08 -9.4  -9.4 ]\n[-1.08 -0.4   3.   -1.08 -9.4  -9.4 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDYTOt1BEnom"
      },
      "source": [
        "# To show that at state 393, how the move evolved\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dwwIlFp67Dg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82f61902-2ad1-47ea-9b53-0177137938e4"
      },
      "source": [
        "action_dict = {0:  \"move south\"\n",
        ",1: \"move north\"\n",
        ",2: \"move east\"\n",
        ",3: \"move west\"\n",
        ",4: \"pickup passenger\"\n",
        ",5: \"dropoff passenger\"\n",
        "}\n",
        "\n",
        "ENV_STATE = env.reset()\n",
        "print(env.render(mode='ansi'))\n",
        "state_memory = [i[ENV_STATE] for i in training_memory]\n",
        "printmd(\"For state **{}**\".format(ENV_STATE))\n",
        "for step, i in enumerate(state_memory):\n",
        "    \n",
        "    if step % 200==0:\n",
        "        choice = np.argmax(i)\n",
        "        printmd(\"for episode in {}, q table action is {} and it will ... **{}**\".format(step*100, choice, action_dict[choice]))\n",
        "        print(i)\n",
        "        print()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n|R: | : :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : : : |\n| | : |\u001b[43m \u001b[0m: |\n|\u001b[34;1mY\u001b[0m| : |B: |\n+---------+\n\n\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "For state **369**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "for episode in 0, q table action is 0 and it will ... **move south**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.24999822  -1.24999945  -1.24999867  -1.24999849 -10.22355492\n -10.24977275]\n\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "for episode in 20000, q table action is 1 and it will ... **move north**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.25  -1.25  -1.25  -1.25 -10.25 -10.25]\n\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "for episode in 40000, q table action is 1 and it will ... **move north**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.25  -1.25  -1.25  -1.25 -10.25 -10.25]\n\n"
          ]
        }
      ]
    },
    {
      "source": [
        "### Running a trained taxi"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "This is a clearer view of the transition between states and the reward that will be received.\n",
        "Notice that, as the reward is consistently high for a trained model."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZWqI-fz4ZnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6142a2-7b73-4355-9d60-678ed116017d"
      },
      "source": [
        "import time\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episode: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        time.sleep(0.8)\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 10 # Try 10 rounds\n",
        "frames = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state]) # deterministic (exploit), not stochastic (explore), only explore in training\n",
        "        env\n",
        "        state, reward, done, info = env.step(action) #gym\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "        \n",
        "        # Put each rendered frame into dict for animation, gym generated\n",
        "        frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'episode': ep, \n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward\n",
        "            }\n",
        "        )\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print_frames(frames)\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Episode: 9\n",
            "Timestep: 123\n",
            "State: 475\n",
            "Action: 5\n",
            "Reward: 20\n",
            "Results after 10 episodes:\n",
            "Average timesteps per episode: 12.3\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "Here, we looked at how Q-table is being used. However, it is a primitive example as we are dealing with finite states for infinite states we would have to rely on a model instead of a table. This is called Q-learning which is not covered here."
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}