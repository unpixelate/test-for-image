{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. ChatBot text_to_text.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python379jvsc74a57bd003cc6c235735ef41ce6e0195c89775c2a43a56af6ff61cfa3ad0c5118b18cb89",
      "display_name": "Python 3.7.9 64-bit ('qe-mini-example': conda)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhwN0XQX4Icu"
      },
      "source": [
        "## Chatbot, Speech and NLP\n",
        "\n"
      ]
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "In this chapter, we will explore the speech to text capabilities with python, then we will assemble a seq2seq LSTM model using Keras Functional API to create a working Chatbot which would answer questions asked to it. You can try integrating both programs together. However, do note that the code we have provided does not integrate both component.\n",
        "\n",
        "Chatbots have become applications themselves. You can choose the field or stream and gather data regarding various questions. We can build a chatbot for an e-commerce webiste or a school website where parents could get information about the school.\n",
        "\n",
        "\n",
        "Messaging platforms like Allo have implemented chatbot services to engage users. The famous [Google Assistant](https://assistant.google.com/), [Siri](https://www.apple.com/in/siri/), [Cortana](https://www.microsoft.com/en-in/windows/cortana) and [Alexa](https://www.alexa.com/) may have been build using simialr models.\n",
        "\n",
        "So, let's start building our Chatbot."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Speech to text"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "#pip install SpeechRecognition\n",
        "#pip install pipwin\n",
        "#pipwin install pyaudio\n",
        "import speech_recognition as sr\n",
        "import sys\n",
        "r = sr.Recognizer()\n",
        "\n",
        "print(\"please say something in 4 seconds... and wait for 4 seconds for the answer.....\")\n",
        "print(\"Accessing Microphone..\")\n",
        "\n",
        "try:\n",
        "    with sr.Microphone() as source:   \n",
        "        r.adjust_for_ambient_noise(source, duration=2)          \n",
        "    # use the default microphone as the audio source, duration higher means environment noisier\n",
        "        print(\"Waiting for you to speak...\")\n",
        "        audio = r.listen(source)                   # listen for the first phrase and extract it into audio data\n",
        "\n",
        "except (ModuleNotFoundError,AttributeError):\n",
        "    print('Please check installation')\n",
        "    sys.exit(0)\n",
        "\n",
        "try:\n",
        "    print(\"You said \" + r.recognize_google(audio))    # recognize speech using Google Speech Recognition\n",
        "except LookupError:                            # speech is unintelligible\n",
        "    print(\"Could not understand audio\")\n",
        "\n",
        "except:\n",
        "    print(\"Please retry...\")\n"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The following code requires `SpeechRecognition`, `pipwin` and `pyaudio`. Please install first before carry out the code. The code adjusts according to the ambient noise which help to caputure what we have said. If there is an error, try adjusting the `duration` parameter in `adjust_for_ambient_noise`. Now we have managed to capture what is said in ` r.recognize_google(audio)`. We will be able to use this can pass it to our chatbot."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm5g4WIG5ym2"
      },
      "source": [
        "### Importing the packages for chatbot\n",
        "\n",
        "We will import [TensorFlow](https://www.tensorflow.org) and our beloved [Keras](https://www.tensorflow.org/guide/keras). Also, we import other modules which help in defining model layers.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgZHR8TO0lFF"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers , activations , models , preprocessing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxiGOLldKOQD"
      },
      "source": [
        "### Preprocessing the data for chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imkdw4os6FI4"
      },
      "source": [
        "#### A) Download the data\n",
        "\n",
        "The dataset hails from [chatterbot/english on Kaggle](https://www.kaggle.com/kausr25/chatterbotenglish).com by [kausr25](https://www.kaggle.com/kausr25). It contains pairs of questions and answers based on a number of subjects like food, history, AI etc.\n",
        "\n",
        "The raw data could be found from this repo -> https://github.com/shubham0204/Dataset_Archives\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i6u8US30ufe"
      },
      "source": [
        "\n",
        "!wget https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true -O chatbot_nlp.zip\n",
        "!unzip chatbot_nlp.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1mDKD_R6Os"
      },
      "source": [
        "#### B) Reading the data from the files\n",
        "\n",
        "We parse each of the `.yaml` files.\n",
        "\n",
        "*   Concatenate two or more sentences if the answer has two or more of them.\n",
        "*   Remove unwanted data types which are produced while parsing the data.\n",
        "*   Append `<START>` and `<END>` to all the `answers`.\n",
        "*   Create a `Tokenizer` and load the whole vocabulary ( `questions` + `answers` ) into it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzTBhga6MiV7"
      },
      "source": [
        "\n",
        "from tensorflow.keras import preprocessing , utils\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "dir_path = 'chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I74UZ4TczNDv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzsaO1YvS-M8"
      },
      "source": [
        "#### C) Preparing data for Seq2Seq model\n",
        "\n",
        "Our model requires three arrays namely `encoder_input_data`, `decoder_input_data` and `decoder_output_data`.\n",
        "\n",
        "For `encoder_input_data` :\n",
        "* Tokenize the `questions`. Pad them to their maximum length.\n",
        "\n",
        "For `decoder_input_data` :\n",
        "* Tokenize the `answers`. Pad them to their maximum length.\n",
        "\n",
        "For `decoder_output_data` :\n",
        "\n",
        "* Tokenize the `answers`. Remove the first element from all the `tokenized_answers`. This is the `<START>` element which we added earlier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AD9ooQKc33"
      },
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append( word )\n",
        "\n",
        "def tokenize( sentences ):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append( tokens )\n",
        "    return tokens_list , vocabulary\n",
        "\n",
        "#p = tokenize( questions + answers )\n",
        "#model = Word2Vec( p[ 0 ] ) \n",
        "\n",
        "#embedding_matrix = np.zeros( ( VOCAB_SIZE , 100 ) )\n",
        "#for i in range( len( tokenizer.word_index ) ):\n",
        "    #embedding_matrix[ i ] = model[ vocab[i] ]\n",
        "\n",
        "# encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
        "encoder_input_data = np.array( padded_questions )\n",
        "print( encoder_input_data.shape , maxlen_questions )\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )\n",
        "\n",
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbGyrsEN2shZ"
      },
      "source": [
        "tokenized_questions[0],tokenized_questions[1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr-VaYWa2z3s"
      },
      "source": [
        "padded_questions[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SwY3T139l19"
      },
      "source": [
        "### Defining the Encoder-Decoder model\n",
        "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
        "\n",
        "\n",
        "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
        "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
        "*   LSTM layer : Provide access to Long-Short Term cells.\n",
        "\n",
        "Working : \n",
        "\n",
        "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
        "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
        "3.   These states are set in the LSTM cell of the decoder.\n",
        "4.   The decoder_input_data comes in through the Embedding layer.\n",
        "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
        "\n",
        "\n",
        "\n",
        "<center><img style=\"float: center;\" src=\"https://cdn-images-1.medium.com/max/1600/1*bnRvZDDapHF8Gk8soACtCQ.gif\"></center>\n",
        "\n",
        "\n",
        "Image credits to [Hackernoon](https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gUYtOwv21rt"
      },
      "source": [
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9g_8sR7WWf3"
      },
      "source": [
        "### Training the model\n",
        "We train the model for a number of epochs with `RMSprop` optimizer and `categorical_crossentropy` loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N74NZnfo3Id-"
      },
      "source": [
        "\n",
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150, verbose=0 ) \n",
        "model.save( 'model.h5' ) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVmXxaxZKYD"
      },
      "source": [
        "output = model.predict([encoder_input_data[0,np.newaxis], decoder_input_data[0,np.newaxis]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6RAsFv-c_IW"
      },
      "source": [
        "output[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD8Y6O0rcogf"
      },
      "source": [
        "np.argmax(output[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2L0AKHHeTaU"
      },
      "source": [
        "tokenizer_dict = { tokenizer.word_index[i]:i for i in tokenizer.word_index}\r\n",
        "tokenizer_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QKXJla5dZAG"
      },
      "source": [
        "tokenizer_dict[np.argmax(output[0][1])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip9AXEupdZVn"
      },
      "source": [
        "tokenizer_dict[np.argmax(output[0][2])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3G6FJ0xZ6zw"
      },
      "source": [
        "output = model.predict([encoder_input_data[0,np.newaxis], decoder_input_data[0,np.newaxis]])\r\n",
        "sampled_word_indexes = np.argmax(output[0],1)\r\n",
        "sentence = \"\"\r\n",
        "maxlen_answers = 74\r\n",
        "for sampled_word_index in sampled_word_indexes:\r\n",
        "    sampled_word = None\r\n",
        "    sampled_word = tokenizer_dict[sampled_word_index]\r\n",
        "    sentence += ' {}'.format( sampled_word )\r\n",
        "    if sampled_word == 'end' or len(sentence.split()) > maxlen_answers:\r\n",
        "        break\r\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBvl8ABXbhSS"
      },
      "source": [
        "def print_train_result(index):\r\n",
        "    print(f\"Question is : {questions[index]}\")\r\n",
        "    print(f\"Answer is : {answers[index]}\")\r\n",
        "    output = model.predict([encoder_input_data[index,np.newaxis], decoder_input_data[index,np.newaxis]])\r\n",
        "    sampled_word_indexes = np.argmax(output[0],1)\r\n",
        "    sentence = \"\"\r\n",
        "    maxlen_answers = 74\r\n",
        "    for sampled_word_index in sampled_word_indexes:\r\n",
        "        sampled_word = None\r\n",
        "        sampled_word = tokenizer_dict[sampled_word_index]\r\n",
        "        sentence += ' {}'.format( sampled_word )\r\n",
        "        if sampled_word == 'end' or len(sentence.split()) > maxlen_answers:\r\n",
        "            break\r\n",
        "    print(f\"Model prediction: {sentence}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kb2Gj3ib-fW"
      },
      "source": [
        "print_train_result(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMt97fn7cCLe"
      },
      "source": [
        "print_train_result(55)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSXs2PHIcbzu"
      },
      "source": [
        "print_train_result(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sOLQr0M-lAe"
      },
      "source": [
        "### Defining inference models\n",
        "We create inference models which help in predicting answers.\n",
        "\n",
        "**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n",
        "\n",
        "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u5DE4qo3Mf2"
      },
      "source": [
        "\n",
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxZp0ZRy-6dA"
      },
      "source": [
        "### Talking with our Chatbot\n",
        "\n",
        "First, we define a method `str_to_tokens` which converts `str` questions to Integer tokens with padding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P_wDD554q9O"
      },
      "source": [
        "\n",
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djEPrfJBmZE-"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.   First, we take a question as input and predict the state values using `enc_model`.\n",
        "2.   We set the state values in the decoder's LSTM.\n",
        "3.   Then, we generate a sequence which contains the `<start>` element.\n",
        "4.   We input this sequence in the `dec_model`.\n",
        "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
        "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer length.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zBmN8qB3O-e"
      },
      "source": [
        "\n",
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Sample code"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "#https://cloudconvert.com/m4a-to-wav\n",
        "Path = \"C:/Users/User/Dropbox/TT Library/AI Model/Speech & Chatbot & NLP/Recording.wav\"\n",
        "\n",
        "Audio(Path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wave\n",
        "\n",
        "audio = wave.open(Path)\n",
        "\n",
        "from scipy.io import wavfile\n",
        "\n",
        "fs, x = wavfile.read(Path)\n",
        "print('Reading with scipy.io.wavfile.read:', x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "r = sr.Recognizer()\n",
        "audio1 = sr.AudioFile(Path)\n",
        "with audio1 as source:\n",
        "    audio = r.record(source)\n",
        "\n",
        "print(\"Speech to text : \" + r.recognize_google(audio))    "
      ]
    }
  ]
}