{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "source": [
    "**Learning outcomes:**\n",
    "- Learn the difference between classification and regression. Be able to differentiate between classification and regression problems \n",
    "- Learn and apply basic models for classification using sklearn and keras.\n",
    "Learn techniques to achieve better classification result\n",
    "\n",
    "\n",
    "We have learnt about regression previously. Now, let's take a look at classificaiton. Fundamentally, classification is about predicting a label and regression is about predicting a quantity.\n",
    "\n",
    "Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y). The output variables are often called labels or categories. The mapping function predicts the class or category for a given observation.\n",
    "\n",
    "For example, an email of text can be classified as belonging to one of two classes: “spam“ and “not spam“. A classification can have real-valued or discrete input variables.\n",
    "\n",
    "Here are different types of classification problem: \n",
    "- A problem with two classes is often called a two-class or binary classification problem.\n",
    "- A problem with more than two classes is often called a multi-class classification problem.\n",
    "- A problem where an example is assigned multiple classes is called a multi-label classification problem.\n",
    "\n",
    "It is common for classification models to predict a continuous value as the probability of a given example belonging to each output class. The probabilities can be interpreted as the likelihood or confidence of a given example belonging to each class. A predicted probability can be converted into a class value by selecting the class label that has the highest probability.\n",
    "\n",
    "For example, a specific email of text may be assigned the probabilities of 0.1 as being “spam” and 0.9 as being “not spam”. We can convert these probabilities to a class label by selecting the “not spam” label as it has the highest predicted likelihood.\n",
    "\n",
    "There are many ways to estimate the skill of a classification predictive model, but perhaps the most common is to calculate the classification accuracy.\n",
    "\n",
    "The classification accuracy is the percentage of correctly classified examples out of all predictions made.\n",
    "\n",
    "For example, if a classification predictive model made 5 predictions and 3 of them were correct and 2 of them were incorrect, then the classification accuracy of the model based on just these predictions would be:\n",
    "\n",
    "```python \n",
    "accuracy = correct predictions / total predictions * 100\n",
    "accuracy = 3 / 5 * 100\n",
    "accuracy = 60%\n",
    "``` \n",
    "An algorithm that is capable of learning a classification predictive model is called a classification algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3088,
     "status": "ok",
     "timestamp": 1605503362050,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "DWfKZ2kHulgs",
    "outputId": "9505bcd0-6094-4602-b912-4965b23a935b"
   },
   "source": [
    "<b>Dataset: \"Alumni Giving Regression (Edited).csv\" </b>\n",
    "\n",
    "You can obtain the data set from this link https://www.dropbox.com/s/ggxo241uog06yhj/Diabetes%20%28Edited%29.csv?dl=0\n",
    "\n",
    "Also, you may run the following code in order to download the dataset in `google colab`: \n",
    "\n",
    "```\n",
    "!wget https://www.dropbox.com/s/ggxo241uog06yhj/Diabetes%20%28Edited%29.csv?dl=0 -O --quiet \"Diabetes (Edited).csv\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "source": [
    "Firstly, we will work on preprocessing the data. For numerical data, often we would preprocess the data by scaling it. In our example, we apply standard scalar, a popular preprocessing technique.\n",
    "\n",
    "Standardization is a transformation that centers the data by removing the mean value of each feature and then scale it by dividing (non-constant) features by their standard deviation. After standardizing data the mean will be zero and the standard deviation one.\n",
    "\n",
    "Standardization can drastically improve the performance of models. For instance, many elements used in the objective function of a learning algorithm assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "Here the code that does the scaling is as follows:\n",
    "\n",
    "```python\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "Notice that we are using the scalar fitted on our `X_train` to transform values in X_test. This is to ensure that our model does not learn from the testing data. Usually, we would split our data before applying scaling. It is a bad practice to do scaling on the full data set.\n",
    "\n",
    "Apart from standard scaling we can use other scalar such as `MinMaxScalar`. `feature_range` refers to the highest and lowest values after scaling. By default,\n",
    "`feature_range' is -1 to 1. However, this range may proof to be too small as changes in our variable would be compressed to maximum of -1 to 1.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-3,3))\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      A    B   C   D    E     F      G   H  I\n0     6  148  72  35    0  33.6  0.627  50  1\n1     1   85  66  29    0  26.6  0.351  31  0\n2     8  183  64   0    0  23.3  0.672  32  1\n3     1   89  66  23   94  28.1  0.167  21  0\n4     0  137  40  35  168  43.1  2.288  33  1\n..   ..  ...  ..  ..  ...   ...    ...  .. ..\n763  10  101  76  48  180  32.9  0.171  63  0\n764   2  122  70  27    0  36.8  0.340  27  0\n765   5  121  72  23  112  26.2  0.245  30  0\n766   1  126  60   0    0  30.1  0.349  47  1\n767   1   93  70  31    0  30.4  0.315  23  0\n\n[768 rows x 9 columns]\n                A           B           C           D           E           F  \\\ncount  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \nmean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \nstd      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \nmax     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n\n                G           H           I  \ncount  768.000000  768.000000  768.000000  \nmean     0.471876   33.240885    0.348958  \nstd      0.331329   11.760232    0.476951  \nmin      0.078000   21.000000    0.000000  \n25%      0.243750   24.000000    0.000000  \n50%      0.372500   29.000000    0.000000  \n75%      0.626250   41.000000    1.000000  \nmax      2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "Y_position = 8\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "df = pd.read_csv('Diabetes (Edited).csv')\n",
    "print(df)\n",
    "# summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "X = df.iloc[:,0:Y_position]\n",
    "Y = df.iloc[:,Y_position]\n",
    "\n",
    "# create model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.40, random_state=2020)\n",
    "\n",
    "#scaling to around -2 to 2 (Z)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "source": [
    "In order to reduce code duplication as seen in the chapter on `Regression`. We can abstract the model and create a function to help us train and predict. Here is the explaination for the code:\n",
    "\n",
    "```python\n",
    "model.fit(scaled_X_train, y_train)\n",
    "```\n",
    "We train the model using `scaled_X_train` and provide its label `y_train`\n",
    "\n",
    "```python\n",
    "y_predicted = model3.predict(scaled_X_test)\n",
    "```\n",
    "We predict the model on our testing data and store its result in the variable `y_predicted`\n",
    "\n",
    "```python\n",
    "cm_test = confusion_matrix(y_test,y_pred)\n",
    "```\n",
    "We create a confusion matrix given our y_test and y_pred. And what is a confusion matrix?\n",
    "\n",
    "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n",
    "\n",
    "- Expected down the side: Each row of the matrix corresponds to a predicted class.\n",
    "- Predicted across the top: Each column of the matrix corresponds to an actual class\n",
    "\n",
    "```python \n",
    " acc_test = (cm_test[0,0] + cm_test[1,1]) / sum(sum(cm_test))\n",
    " ```\n",
    "Lastly, this code calculates the accuracy for us. Accuracy is the number of correctly predicted data points out of all the data points. More formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives. These values are the outputs of a confusion matrix. \n",
    "\n",
    "Here, we are assuming a binary classification problem. For multiclass classification problem, I would highly recommand using sklearn's `accuracy` function for its calculation.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_using_model(model_name= \"\",model=None):\n",
    "    model.fit(scaled_X_train, y_train)\n",
    "    y_pred_train = model.predict(scaled_X_train)\n",
    "    cm_train = confusion_matrix(y_train,y_pred_train)\n",
    "    print(model_name)\n",
    "    print(\"================================\")\n",
    "    print(\"Training confusion matrix: \")\n",
    "    print(cm_train)\n",
    "    acc_train = (cm_train[0,0] + cm_train[1,1]) / sum(sum(cm_train))\n",
    "    print(\"TrainSet: Accurarcy %.2f%%\" % (acc_train*100))\n",
    "    print(\"================================\")\n",
    "    y_pred = model.predict(scaled_X_test)\n",
    "    cm_test = confusion_matrix(y_test,y_pred)\n",
    "    print(cm_test)\n",
    "    acc_test = (cm_test[0,0] + cm_test[1,1]) / sum(sum(cm_test))\n",
    "    print(\"Testset: Accurarcy %.2f%%\" % (acc_test*100))\n",
    "    print(\"================================\")"
   ]
  },
  {
   "source": [
    "### Logistic regression\n",
    "\n",
    "<b> Why not use linear regression </b>\n",
    "\n",
    "Suppose we have a data of tumor size vs its malignancy. As it is a classification problem, if we plot, we can see, all the values will lie on 0 and 1. And if we fit best found regression line, by assuming the threshold at 0.5, we can do line pretty reasonable job.\n",
    "\n",
    "![Logistic regression](src/logistic1.png)\n",
    "\n",
    "We can decide the point on the x axis from where all the values lie to its left side are considered as negative class and all the values lie to its right side are positive class.\n",
    "\n",
    "![Logistic regression](src/logistic2.jpeg)\n",
    "\n",
    "But what if there is an outlier in the data. Things would get pretty messy. For example, for 0.5 threshold,\n",
    "\n",
    "![Logistic regression](src/logistic3.png)\n",
    "\n",
    "If we fit best found regression line, it still won’t be enough to decide any point by which we can differentiate classes. It will put some positive class examples into negative class. The green dotted line (Decision Boundary) is dividing malignant tumors from benign tumors but the line should have been at a yellow line which is clearly dividing the positive and negative examples. So just a single outlier is disturbing the whole linear regression predictions. And that is where logistic regression comes into a picture.\n",
    "\n",
    "As discussed earlier, to deal with outliers, Logistic Regression uses Sigmoid function.\n",
    "An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a Sigmoid function, which takes any real value between zero and one. It is defined as\n",
    "Image for post\n",
    "\n",
    "![Logistic regression](src/log_form.png)\n",
    "\n",
    "And if we plot it, the graph will be S curve,\n",
    "\n",
    "![Logistic regression](src/logistic4.png)\n",
    "\n",
    "Now, when logistic regression model come across an outlier, it will take care of it.\n",
    "\n",
    "![Logistic regression](src/logistic5.png)\n",
    "\n",
    "<b> Another way of looking at logistic regression </b>\n",
    "\n",
    "![Logistic regression](src/logistic1.png)\n",
    "\n",
    "Consider the case where we are looking at a classification problem and our output is probability. Our output is from 0 to 1 which represents the probability that the event has occured. Using linear regression would result in output from 1 to infinity, which when mapped to a `sigmod` function goes very well into 0 to 1 depending on the output of linear regression. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11525,
     "status": "ok",
     "timestamp": 1605503375075,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "Ce1il03S6_Te",
    "outputId": "5c8759b8-ad8e-4978-a44b-d937106c2ae1",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Regression\n================================\n[[274  31]\n [ 62  93]]\nRegression TrainSet: Accurarcy 79.78%\n================================\n[[172  23]\n [ 53  60]]\nRegression Testset: Accurarcy 75.32%\n================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "linear_classifier = linear_model.LogisticRegression(random_state=123)\n",
    "linear_classifier.fit(scaled_X_train, y_train)\n",
    "y_pred_train1 = linear_classifier.predict(scaled_X_train)\n",
    "cm1_train = confusion_matrix(y_train,y_pred_train1)\n",
    "print(\"Regression\")\n",
    "print(\"================================\")\n",
    "print(cm1_train)\n",
    "acc_train1 = (cm1_train[0,0] + cm1_train[1,1]) / sum(sum(cm1_train))\n",
    "print(\"Regression TrainSet: Accurarcy %.2f%%\" % (acc_train1*100))\n",
    "print(\"================================\")\n",
    "y_pred1 = linear_classifier.predict(scaled_X_test)\n",
    "cm1 = confusion_matrix(y_test,y_pred1)\n",
    "print(cm1)\n",
    "acc1 = (cm1[0,0] + cm1[1,1]) / sum(sum(cm1))\n",
    "print(\"Regression Testset: Accurarcy %.2f%%\" % (acc1*100))\n",
    "print(\"================================\")"
   ]
  },
  {
   "source": [
    "Sample result:\n",
    "```python\n",
    "================================\n",
    "[[274  31]\n",
    " [ 62  93]]\n",
    "Regression TrainSet: Accurarcy 79.78%\n",
    "================================\n",
    "```\n",
    "``` python\n",
    "[[274  31]\n",
    " [ 62  93]]\n",
    "```\n",
    "\n",
    "Here is an example of a confusion matrix in python.\n",
    "\n",
    "- 274 is when the Actual class is True and Predicted class is True.\n",
    "- 31 is when the Actual class is True and Predicted class is False\n",
    "- 62 is when the Actual class is False and Predicted class is True\n",
    "- 93 is when the Actual class is False and Predicted class is False"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Improvement to our code**\n",
    "\n",
    "Recall we have written a helper function to help us to capture the logic of training the model,prediciting the output and printing the train and test accurary as well as confusion matrix? Let put it to use here!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression\n================================\nTraining confusion matrix: \n[[274  31]\n [ 62  93]]\nTrainSet: Accurarcy 79.78%\n================================\n[[172  23]\n [ 53  60]]\nTestset: Accurarcy 75.32%\n================================\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_using_model('Logistic Regression', linear_classifier)"
   ]
  },
  {
   "source": [
    "We have managed to reduce multiple lines of code to a succinct function call. This is a huge improvement in terms of code maintenance and code changes. If we need to change any of our code, we only have to apply it on our `train_and_predict_using_model` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Decision Tree and Random Forest\n",
    "\n",
    "The code and intuition behind Decision Tree and Random Forest is similar to that in regression. Thus, we will not be delving deeper into both models.\n",
    "\n",
    "The code is as follows:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Decision Tree Classifier\n================================\nTraining confusion matrix: \n[[274  31]\n [ 62  93]]\nTrainSet: Accurarcy 79.78%\n================================\n[[172  23]\n [ 53  60]]\nTestset: Accurarcy 75.32%\n================================\n\n\n\nRandom Forest Classifier\n================================\nTraining confusion matrix: \n[[290  15]\n [ 96  59]]\nTrainSet: Accurarcy 75.87%\n================================\n[[184  11]\n [ 80  33]]\nTestset: Accurarcy 70.45%\n================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decision_tree_clf = tree.DecisionTreeClassifier()\n",
    "train_and_predict_using_model('Decision Tree Classifier', linear_classifier)\n",
    "\n",
    "print(\n",
    "    '\\n\\n'\n",
    ")\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "train_and_predict_using_model('Random Forest Classifier', rf_clf)"
   ]
  },
  {
   "source": [
    "### Neural network\n",
    "\n",
    "Lastly, we have neural network. Similar to logistic regression, we have to map our output from `-inf` to `inf` to `0` to `1`. Here, we will have to add a `Dense` layer with a `sigmoid` activation function. For multiclass, we should use a `softmax` activation function.\n",
    "\n",
    "```python\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "Here, we added a last layer mapping to a sigmoid function. Notice that we have 1 neuron in this layer as we would like to have 1 predicition. This might be different for multi-class and we should always check out the documentation.\n",
    "\n",
    "```python\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "```\n",
    "Also, we would need to tell the model that we need to use a different loss function. Here, for binary problem (Yes/No). `binary_crossentropy` is the way to go. For multiclass problem, we might need to use `categorical_crossentropy` as the loss function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "460/460 [==============================] - 0s 63us/step\n",
      "Neural Network Trainset: \n",
      "accuracy: 71.09%\n",
      "==================================\n",
      "==================================\n",
      "Neural Network on testset confusion matrix\n",
      "[[177  18]\n",
      " [ 78  35]]\n",
      "Neural Network on TestSet: Accuracy 68.83%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Neural network\n",
    "#https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=Y_position, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "# https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(scaled_X_train, y_train, epochs=1, batch_size=20, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(scaled_X_train, y_train)\n",
    "\n",
    "print(\"Neural Network Trainset: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "predictions = model.predict(scaled_X_test)\n",
    "\n",
    "y_pred = (predictions > 0.5)\n",
    "y_pred = y_pred*1 #convert to 0,1 instead of True False\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"==================================\")\n",
    "print(\"==================================\")\n",
    "print(\"Neural Network on testset confusion matrix\")\n",
    "print(cm)\n",
    "\n",
    "## Get accurary from Confusion matrix\n",
    "## Position 0,0 and 1,1 are the correct predictions \n",
    "acc = (cm[0,0] + cm[1,1]) / sum(sum(cm))\n",
    "print(\"Neural Network on TestSet: Accuracy %.2f%%\" % (acc*100))\n"
   ]
  },
  {
   "source": [
    "From above, notice that the training accuracy is at 71% which might be a case of underfitting. To improve our model, we can always increase the number of neurons / layer or increase the epoch for training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "460/460 [==============================] - 0s 126us/step\n",
      "Neural Network Trainset: \n",
      "accuracy: 99.57%\n",
      "==================================\n",
      "==================================\n",
      "Neural Network on testset confusion matrix\n",
      "[[152  43]\n",
      " [ 43  70]]\n",
      "Neural Network on TestSet: Accuracy 72.08%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Neural network\n",
    "#https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=Y_position, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "# https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(scaled_X_train, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(scaled_X_train, y_train)\n",
    "\n",
    "print(\"Neural Network Trainset: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "predictions = model.predict(scaled_X_test)\n",
    "\n",
    "y_pred = (predictions > 0.5)\n",
    "y_pred = y_pred*1 #convert to 0,1 instead of True False\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"==================================\")\n",
    "print(\"==================================\")\n",
    "print(\"Neural Network on testset confusion matrix\")\n",
    "print(cm)\n",
    "\n",
    "## Get accurary from Confusion matrix\n",
    "## Position 0,0 and 1,1 are the correct predictions \n",
    "acc = (cm[0,0] + cm[1,1]) / sum(sum(cm))\n",
    "print(\"Neural Network on TestSet: Accuracy %.2f%%\" % (acc*100))\n"
   ]
  },
  {
   "source": [
    "Now, our accurary on training has reached 99%. However, accurary of test is still lower. This might be because of testing dataset differing from training dataset or overfitting. For overfitting, we will look at some regularization techniques. For now, adding `Dropout` layer and reducing training `epoch` would work just fine.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Js0ulagcnKNW"
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNzY3eeQn0KI"
   },
   "source": [
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "```python\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLZ09qP3neCJ"
   },
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvCRjwsAoKvh"
   },
   "source": [
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "```python\n",
    "class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nSZP7j1dIBJ"
   },
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1605503375076,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "whZbWt0hdIBK",
    "outputId": "474dbc45-9ff3-47a1-82b9-2e13c6a819d5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature (Column index) 1 (0.307004)\n",
      "2. feature (Column index) 7 (0.237150)\n",
      "3. feature (Column index) 0 (0.129340)\n",
      "4. feature (Column index) 5 (0.129255)\n",
      "5. feature (Column index) 6 (0.069927)\n",
      "6. feature (Column index) 4 (0.055137)\n",
      "7. feature (Column index) 2 (0.044458)\n",
      "8. feature (Column index) 3 (0.027729)\n"
     ]
    }
   ],
   "source": [
    "RF = model3\n",
    "importances = RF.feature_importances_\n",
    "std = numpy.std([tree.feature_importances_ for tree in RF.estimators_],\n",
    "             axis=0)\n",
    "indices = numpy.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature (Column index) %s (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "executionInfo": {
     "elapsed": 2500,
     "status": "ok",
     "timestamp": 1605503378930,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "BuDxnhWGdIBh",
    "outputId": "948a76f9-b896-4c23-93e4-e16afd8c4b16"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "boxplot = pd.DataFrame(dataset).boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXFywaNXvGxt"
   },
   "source": [
    "### Remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1969,
     "status": "ok",
     "timestamp": 1605503382822,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "0vDI9ZQJvDq2",
    "outputId": "37ffc10e-1610-43d6-dbdf-c2db299825a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 9), (760, 9))"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "quantile = df[4].quantile(0.99)\n",
    "df1 = df[df[4] < quantile]\n",
    "df.shape, df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGgSwffuvIRV"
   },
   "outputs": [],
   "source": [
    "df1 = df1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0oHOT1nvTkH"
   },
   "source": [
    "### Use top 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10244,
     "status": "ok",
     "timestamp": 1605503396370,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "VRvAbgUivSsX",
    "outputId": "e10a8cc8-e4d9-4a79-dab3-d977a5d62110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 7 0]\n",
      "Regression\n",
      "================================\n",
      "[[361  46]\n",
      " [105 102]]\n",
      "Regression TrainSet: Accurarcy 75.41%\n",
      "================================\n",
      "[[82 11]\n",
      " [30 31]]\n",
      "Regression Testset: Accurarcy 73.38%\n",
      "================================\n",
      "================================\n",
      "================================\n",
      "Decision Tree\n",
      "================================\n",
      "[[407   0]\n",
      " [  0 207]]\n",
      "Decsion Tree TrainSet: Accurarcy 100.00%\n",
      "================================\n",
      "[[68 25]\n",
      " [32 29]]\n",
      "Decision Tree Testset: Accurarcy 62.99%\n",
      "================================\n",
      "================================\n",
      "================================\n",
      "Random Forest\n",
      "================================\n",
      "[[377  30]\n",
      " [128  79]]\n",
      "Random Forest TrainSet: Accurarcy 74.27%\n",
      "================================\n",
      "[[87  6]\n",
      " [40 21]]\n",
      "Random Forest Testset: Accurarcy 70.13%\n",
      "================================\n",
      "================================\n",
      "================================\n",
      "Xgboost\n",
      "================================\n",
      "[[389  18]\n",
      " [ 58 149]]\n",
      "Xgboost TrainSet: Accurarcy 87.62%\n",
      "==================================\n",
      "Xgboost on testset confusion matrix\n",
      "[[80 13]\n",
      " [29 32]]\n",
      "Xgboost on TestSet: Accuracy 72.73%\n",
      "==================================\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.7671\n",
      "Neural Network Trainset: \n",
      "accuracy: 76.71%\n",
      "==================================\n",
      "==================================\n",
      "Neural Network on testset confusion matrix\n",
      "[[81 12]\n",
      " [29 32]]\n",
      "Neural Network on TestSet: Accuracy 73.38%\n"
     ]
    }
   ],
   "source": [
    "indices_top3 = indices[:3]\n",
    "print(indices_top3)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"Diabetes (Edited).csv\", delimiter=\",\")\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "Y_position = 8\n",
    "TOP_N_FEATURE = 3\n",
    "\n",
    "X = dataset[:,indices_top3]\n",
    "Y = dataset[:,Y_position]\n",
    "# create model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=2020)\n",
    "\n",
    "\n",
    "#scaling to around -2 to 2 (Z)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "#Model 1 : linear regression\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "#intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "#verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "linear_classifier = linear_model.LogisticRegression(random_state=123)\n",
    "linear_classifier.fit(scaled_X_train, y_train)\n",
    "y_pred_train1 = linear_classifier.predict(scaled_X_train)\n",
    "cm1_train = confusion_matrix(y_train,y_pred_train1)\n",
    "print(\"Regression\")\n",
    "print(\"================================\")\n",
    "print(cm1_train)\n",
    "acc_train1 = (cm1_train[0,0] + cm1_train[1,1]) / sum(sum(cm1_train))\n",
    "print(\"Regression TrainSet: Accurarcy %.2f%%\" % (acc_train1*100))\n",
    "print(\"================================\")\n",
    "y_pred1 = linear_classifier.predict(scaled_X_test)\n",
    "cm1 = confusion_matrix(y_test,y_pred1)\n",
    "print(cm1)\n",
    "acc1 = (cm1[0,0] + cm1[1,1]) / sum(sum(cm1))\n",
    "print(\"Regression Testset: Accurarcy %.2f%%\" % (acc1*100))\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "\n",
    "\n",
    "#Model 2: decision tree\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "#class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "#min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(scaled_X_train, y_train)\n",
    "y_pred_train2 = clf.predict(scaled_X_train)\n",
    "cm2_train = confusion_matrix(y_train,y_pred_train2)\n",
    "print(\"Decision Tree\")\n",
    "print(\"================================\")\n",
    "print(cm2_train)\n",
    "acc_train2 = (cm2_train[0,0] + cm2_train[1,1]) / sum(sum(cm2_train))\n",
    "print(\"Decsion Tree TrainSet: Accurarcy %.2f%%\" % (acc_train2*100))\n",
    "print(\"================================\")\n",
    "y_pred2 = clf.predict(scaled_X_test)\n",
    "cm2 = confusion_matrix(y_test,y_pred2)\n",
    "acc2 = (cm2[0,0] + cm2[1,1]) / sum(sum(cm2))\n",
    "print(cm2)\n",
    "print(\"Decision Tree Testset: Accurarcy %.2f%%\" % (acc2*100))\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "\n",
    "\n",
    "#Model 3 random forest\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, \n",
    "#min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "#max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "#n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)[source]\n",
    "\n",
    "model3 = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "model3.fit(scaled_X_train, y_train)\n",
    "y_predicted3 = model3.predict(scaled_X_test)\n",
    "\n",
    "y_pred_train3 = model3.predict(scaled_X_train)\n",
    "cm3_train = confusion_matrix(y_train,y_pred_train3)\n",
    "print(\"Random Forest\")\n",
    "print(\"================================\")\n",
    "print(cm3_train)\n",
    "acc_train3 = (cm3_train[0,0] + cm3_train[1,1]) / sum(sum(cm3_train))\n",
    "print(\"Random Forest TrainSet: Accurarcy %.2f%%\" % (acc_train3*100))\n",
    "print(\"================================\")\n",
    "y_pred3 = model3.predict(scaled_X_test)\n",
    "cm_test3 = confusion_matrix(y_test,y_pred3)\n",
    "print(cm_test3)\n",
    "acc_test3 = (cm_test3[0,0] + cm_test3[1,1]) / sum(sum(cm_test3))\n",
    "print(\"Random Forest Testset: Accurarcy %.2f%%\" % (acc_test3*100))\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "\n",
    "#Model 4: XGBoost\n",
    "\n",
    "print(\"Xgboost\")\n",
    "print(\"================================\")\n",
    "#class sklearn.ensemble.GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, \n",
    "#subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "#max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, \n",
    "#verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, \n",
    "#n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)[source]\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "model4 = GradientBoostingClassifier(random_state=0)\n",
    "model4.fit(scaled_X_train, y_train)\n",
    "y_pred_train4 = model4.predict(scaled_X_train)\n",
    "cm4_train = confusion_matrix(y_train,y_pred_train4)\n",
    "print(cm4_train)\n",
    "acc_train4 = (cm4_train[0,0] + cm4_train[1,1]) / sum(sum(cm4_train))\n",
    "print(\"Xgboost TrainSet: Accurarcy %.2f%%\" % (acc_train4*100))\n",
    "predictions = model4.predict(scaled_X_test)\n",
    "y_pred4 = (predictions > 0.5)\n",
    "y_pred4 =y_pred4*1 #convert to 0,1 instead of True False\n",
    "cm4 = confusion_matrix(y_test, y_pred4)\n",
    "print(\"==================================\")\n",
    "print(\"Xgboost on testset confusion matrix\")\n",
    "print(cm4)\n",
    "acc4 = (cm4[0,0] + cm4[1,1]) / sum(sum(cm4))\n",
    "print(\"Xgboost on TestSet: Accuracy %.2f%%\" % (acc4*100))\n",
    "print(\"==================================\")\n",
    "\n",
    "#Model 5: neural network\n",
    "#https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=TOP_N_FEATURE, activation='relu'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile mode\n",
    "# https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=0)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "#print(scores)\n",
    "print(\"Neural Network Trainset: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "predictions5 = model.predict(X_test)\n",
    "#print(predictions)\n",
    "#print('predictions shape:', predictions.shape)\n",
    "\n",
    "y_pred5 = (predictions5 > 0.5)\n",
    "y_pred5 = y_pred5*1 #convert to 0,1 instead of True False\n",
    "cm5 = confusion_matrix(y_test, y_pred5)\n",
    "print(\"==================================\")\n",
    "print(\"==================================\")\n",
    "print(\"Neural Network on testset confusion matrix\")\n",
    "print(cm5)\n",
    "\n",
    "## Get accurary from Confusion matrix\n",
    "## Position 0,0 and 1,1 are the correct predictions \n",
    "acc5 = (cm5[0,0] + cm5[1,1]) / sum(sum(cm5))\n",
    "print(\"Neural Network on TestSet: Accuracy %.2f%%\" % (acc5*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAgMlQOWhJjA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F3EhgupiKda"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1605504719252,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "AyVH_vlnh_HQ",
    "outputId": "3c5493cb-d929-43cf-ab32-743abdcd6595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classifier)\n",
      "================================\n",
      "Training confusion matrix: \n",
      "[[361  46]\n",
      " [101 106]]\n",
      "TrainSet: Accurarcy 76.06%\n",
      "================================\n",
      "[[84  9]\n",
      " [31 30]]\n",
      "Testset: Accurarcy 74.03%\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "train_and_predict_using_model(\"SVM (Classifier)\", clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IY2MhXmiuF3"
   },
   "source": [
    "#### Important hyper parameters:\n",
    "\n",
    "For SVM here are some important paramteers to take note of: \n",
    "\n",
    "**Kernel**\n",
    "\n",
    "Kernel Function generally transforms the training set of data so that a non-linear decision surface is able to transformed to a linear equation in a higher number of dimension spaces. Some of the possible paramters are as follow:\n",
    "- rbf\n",
    "- polynomial\n",
    "- sigmoid\n",
    "\n",
    "Here is an illustrated use of a rbf kernel\n",
    "\n",
    "![](./regression/rbf.png)\n",
    "\n",
    "Another important parameter would be class_weight. Here, it is mainly used for unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1605504829647,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "yGfeFhrcitKP",
    "outputId": "01dbb50c-e55b-4d80-998c-92e091bbbdc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (RBF kernel)\n",
      "================================\n",
      "Training confusion matrix: \n",
      "[[361  46]\n",
      " [101 106]]\n",
      "TrainSet: Accurarcy 76.06%\n",
      "================================\n",
      "[[84  9]\n",
      " [31 30]]\n",
      "Testset: Accurarcy 74.03%\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='rbf')\n",
    "train_and_predict_using_model(\"SVM (RBF kernel)\", rbf_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1605504963492,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "ZaHNNSaQiW5B",
    "outputId": "c64a40b9-c46d-46cb-bd0d-62eb2e463524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (polynomial kernel)\n",
      "================================\n",
      "Training confusion matrix: \n",
      "[[393  14]\n",
      " [148  59]]\n",
      "TrainSet: Accurarcy 73.62%\n",
      "================================\n",
      "[[89  4]\n",
      " [47 14]]\n",
      "Testset: Accurarcy 66.88%\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='poly')\n",
    "train_and_predict_using_model(\"SVM (polynomial kernel)\", rbf_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1605504986460,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "lxKK5-FZjKGN",
    "outputId": "255b70c9-794e-434b-c3bc-0e332ab510af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (sigmoid kernel)\n",
      "================================\n",
      "Training confusion matrix: \n",
      "[[320  87]\n",
      " [112  95]]\n",
      "TrainSet: Accurarcy 67.59%\n",
      "================================\n",
      "[[68 25]\n",
      " [35 26]]\n",
      "Testset: Accurarcy 61.04%\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='sigmoid')\n",
    "train_and_predict_using_model(\"SVM (sigmoid kernel)\", rbf_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 787,
     "status": "ok",
     "timestamp": 1605505104291,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "fZ0nBLjcjeo6",
    "outputId": "58e87bfe-acfe-406c-af3f-d2bcbb56de7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM uneven class weight\n",
      "================================\n",
      "Training confusion matrix: \n",
      "[[316  91]\n",
      " [ 75 132]]\n",
      "TrainSet: Accurarcy 72.96%\n",
      "================================\n",
      "[[71 22]\n",
      " [18 43]]\n",
      "Testset: Accurarcy 74.03%\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "# fit the model and get the separating hyperplane using weighted classes\n",
    "wclf = svm.SVC(kernel='linear', class_weight={1:2})\n",
    "train_and_predict_using_model('SVM uneven class weight', wclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKJBjSe0ke7Y"
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability `P(c|x)` from `P(c)`, `P(x)` and `P(x|c)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1605505295588,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "8qphxloNj1D9",
    "outputId": "659f1e95-fbc2-4082-c53e-39266de3141a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "================================\n",
      "Training confusion matrix: \n",
      "[[337  70]\n",
      " [ 93 114]]\n",
      "TrainSet: Accurarcy 73.45%\n",
      "================================\n",
      "[[78 15]\n",
      " [28 33]]\n",
      "Testset: Accurarcy 72.08%\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# maximum likelihood\n",
    "\n",
    "gnb = GaussianNB()\n",
    "train_and_predict_using_model(\"Naive Bayes\", gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1605505859405,
     "user": {
      "displayName": "Zheng Rong",
      "photoUrl": "",
      "userId": "15933078287186103415"
     },
     "user_tz": -480
    },
    "id": "sl0cvWLbkqFU",
    "outputId": "5399d938-b2c3-4af6-afe5-9059775f6a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prior [0.105 0.895] close to your defined prior of [0.1, 0.9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X, y = make_classification(n_samples=1000, weights=[0.1, 0.9])\n",
    "# your GNB estimator\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "\n",
    "print(\"model prior {} close to your defined prior of {}\".format(gnb.class_prior_, [0.1,0.9]))"
   ]
  },
  {
   "source": [
    "### Sample code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"C:/Users/User/Dropbox/TT Library/AI Model/Python/Treynor (Classification).csv\")\n",
    "\n",
    "print(df)\n",
    "\n",
    "df=df.dropna()\n",
    "print(df)\n",
    "\n",
    "for i in df.columns:\n",
    "    df=df[pd.to_numeric(df[i], errors='coerce').notnull()] #make it to null then remove null\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=\"Class\", y=\"size_type\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.barplot(x=\"Class\", y= \"Blend\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "print(df)\n",
    "z_scores = stats.zscore(df.astype(np.float))\n",
    "print(z_scores)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "print(filtered_entries)\n",
    "df = df[filtered_entries]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split X and Y\n",
    "\n",
    "X=df.iloc[:,0:len(df.columns)-1]\n",
    "print(X)\n",
    "Y=df.iloc[:,len(df.columns)-1]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy=pd.get_dummies(X[\"Blend\"])\n",
    "dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.merge(dummy, left_index=True, right_index=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop(\"Blend\", axis=\"columns\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "X[\"return_rating\"]=stats.zscore(X[\"return_rating\"].astype(np.float))\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(Y_train)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = linear_model.LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, Y_train)\n",
    "pred=model.predict(X_train)\n",
    "cm=confusion_matrix(pred, Y_train)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(X_test)\n",
    "cm=confusion_matrix(pred, Y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(Y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "pred=model.predict(X_train)\n",
    "cm=confusion_matrix(pred, Y_train)\n",
    "print(cm)\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(X_test)\n",
    "cm=confusion_matrix(pred, Y_test)\n",
    "print(cm)\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model=RandomForestClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "Y_predict=model.predict(X_train)\n",
    "cm=confusion_matrix(Y_train, Y_predict)\n",
    "print(cm)\n",
    "\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)\n",
    "pred=model.predict(X_test)\n",
    "cm=confusion_matrix(pred, Y_test)\n",
    "print(cm)\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model=GradientBoostingClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "pred = model.predict(X_train)\n",
    "cm=confusion_matrix(Y_train, pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)\n",
    "pred=model.predict(X_test)\n",
    "cm=confusion_matrix(Y_test, pred)\n",
    "print(cm)\n",
    "\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(10, input_dim=len(X_train.columns), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'Adamax', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "score=model.evaluate(X_train, Y_train)\n",
    "print(score[1])\n",
    "score=model.evaluate(X_test, Y_test)\n",
    "print(score[1])\n",
    "\n",
    "pred=model.predict(X_train)\n",
    "pred=np.where(pred>0.5,1,0)\n",
    "cm=confusion_matrix(pred, Y_train)\n",
    "print(cm)\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)\n",
    "\n",
    "pred=model.predict(X_test)\n",
    "pred=np.where(pred>0.5,1,0)\n",
    "cm=confusion_matrix(pred, Y_test)\n",
    "print(cm)\n",
    "accuracy=(cm[0,0]+cm[1,1])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}